name: Set up Cluster on Compute Engine VMs

on:
  workflow_dispatch:
    inputs:
      new_cluster:
        description: "Wait for cluster set up to run startup scripts"
        required: true
        type: boolean
        default: true

env:
  PROJECT_ID: ${{ vars.PROJECT_ID }}
  REGION: us-central1
  ZONE: "us-central1-a"
  MASTER_INSTANCE_NAME: k8s-master-node
  WORKER_INSTANCE_NAME: k8s-worker-node
  MASTER_TEMPLATE_NAME: k8s-master-node
  WORKER_TEMPLATE_NAME: k8s-worker-node
  MASTER_SSH_KEY: ${{ secrets.MASTER_SSH_KEY }}
  WORKER_SSH_KEY: ${{ secrets.WORKER_SSH_KEY }}
  SSH_USER: onukwilip

jobs:
  create-vms:
    name: ğŸ—ï¸ Create VM Instances
    runs-on: ubuntu-latest
    outputs:
      master_ip: ${{ steps.master-ip.outputs.master_ip }}
      worker_ip: ${{ steps.worker-ip.outputs.worker_ip }}
    steps:
      - name: ğŸ§° Set up gcloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: Authenticate for GCP
        id: gcp-auth
        uses: google-github-actions/auth@v0
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT }}

      - name: ğŸš€ Create master VM (if not exists)
        run: |
          if gcloud compute instances describe "$MASTER_INSTANCE_NAME" --zone="$ZONE" &> /dev/null; then
            echo "âœ… Master VM already exists, skipping creation."
          else
            echo "ğŸš€ Creating master VM..."
            gcloud compute instances create "$MASTER_INSTANCE_NAME" \
              --zone="$ZONE" \
              --source-instance-template="projects/${{env.PROJECT_ID}}/regions/${{env.REGION}}/instanceTemplates/${{env.MASTER_TEMPLATE_NAME}}"
          fi

      - name: ğŸš€ Create worker VM (if not exists)
        run: |
          if gcloud compute instances describe "$WORKER_INSTANCE_NAME" --zone="$ZONE" &> /dev/null; then
            echo "âœ… Worker VM already exists, skipping creation."
          else
            echo "ğŸš€ Creating worker VM..."
            gcloud compute instances create "$WORKER_INSTANCE_NAME" \
              --zone="$ZONE" \
              --source-instance-template="projects/${{env.PROJECT_ID}}/regions/${{env.REGION}}/instanceTemplates/${{env.WORKER_TEMPLATE_NAME}}"
          fi

      - name: ğŸŒ Get master public IP
        id: master-ip
        run: |
          IP=$(gcloud compute instances describe "$MASTER_INSTANCE_NAME" \
            --zone="$ZONE" \
            --format="value(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "master_ip=$IP" >> "$GITHUB_OUTPUT"

      - name: ğŸŒ Get worker public IP
        id: worker-ip
        run: |
          IP=$(gcloud compute instances describe "$WORKER_INSTANCE_NAME" \
            --zone="$ZONE" \
            --format="value(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "Worker IP: $IP"
          echo "worker_ip=$IP" >> "$GITHUB_OUTPUT"

      - name: â±ï¸ Wait for VMs to finish startup scripts
        if: ${{ github.event.inputs.new_cluster == 'true' }}
        run: |
          echo "Waiting 6 minutes for VMs to complete startup scripts..."
          sleep 360

  generate-join-command:
    name: ğŸ”‘ Generate kubeadm join command
    needs: create-vms
    runs-on: ubuntu-latest
    outputs:
      join_command: ${{ steps.join.outputs.join_command }}
    steps:
      - name: ğŸ§° Set up gcloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}

      - name: ğŸ“¥ Checkout repo
        uses: actions/checkout@v3

      - name: ğŸ” Write credentials to file and authenticate
        run: |
          echo '${{ secrets.GCP_SERVICE_ACCOUNT }}' > /tmp/key.json
          gcloud auth activate-service-account --key-file=/tmp/key.json
          gcloud config set project ${{ env.PROJECT_ID }}

      - name: ğŸ’» SSH into master and retrieve join command
        id: join
        run: |
          JOIN_CMD=$(gcloud compute ssh ${{ env.SSH_USER }}@${{ env.MASTER_INSTANCE_NAME }} \
            --zone=${{ env.ZONE }} \
            --quiet \
            --command='kubeadm token create --print-join-command' 2>/dev/null | grep '^kubeadm join')

          echo "Join command is: $JOIN_CMD"
          echo "join_command=$JOIN_CMD" >> "$GITHUB_OUTPUT"

  bootstrap-worker:
    name: ğŸ› ï¸ Bootstrap Worker Node
    needs: [create-vms, generate-join-command]
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout repo
        uses: actions/checkout@v3

      - name: ğŸ› Debug Outputs
        run: |
          echo "Resolved Master IP: ${{ needs.create-vms.outputs.master_ip }}"
          echo "Resolved Worker IP: ${{ needs.create-vms.outputs.worker_ip }}"
          echo "Join Command: \"${{ needs.generate-join-command.outputs.join_command }}\""

      - name: ğŸ’» SSH into worker and run script
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ needs.create-vms.outputs.worker_ip }}
          username: ${{ env.SSH_USER }}
          key: ${{ env.WORKER_SSH_KEY }}
          script: |
            echo "ğŸš€ Bootstrapping Worker Node..."
            sudo chmod +x ~/online-auction-kubernetes/self-managed/bootstrap-worker-node.sh

            if [ ! -f /etc/kubernetes/kubelet.conf ]; then
              echo "ğŸ”— Node not yet joined to cluster. Proceeding with join..."
              sudo ~/online-auction-kubernetes/self-managed/bootstrap-worker-node.sh "${{ needs.generate-join-command.outputs.join_command }}"
            else
              echo "âœ… Worker node is already part of the cluster. Skipping join step."
            fi

  setup-cloud-controller:
    name: â˜ï¸ Set up Cloud Controller Manager
    needs: [bootstrap-worker, create-vms]
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout repo
        uses: actions/checkout@v3

      - name: ğŸ’» SSH into master and run CCM script
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ needs.create-vms.outputs.master_ip }}
          username: ${{ env.SSH_USER }}
          key: ${{ env.MASTER_SSH_KEY }}
          script: |
            echo "Outside Script"

            echo "PROJECT_ID: ${{ env.PROJECT_ID }}"
            echo "ZONE: ${{ env.ZONE }}"
            echo "MASTER_NODE: ${{ env.MASTER_INSTANCE_NAME }}"
            echo "WORKER_NODES: ${{ env.WORKER_TEMPLATE_NAME }}"

            sudo chmod +x ~/online-auction-kubernetes/self-managed/setup-gcp-ccm.sh

            export PROJECT_ID=${{ env.PROJECT_ID }} \
            export ZONE=${{ env.ZONE }} \
            export MASTER_NODE=${{ env.MASTER_INSTANCE_NAME }} \
            export WORKER_NODES=${{ env.WORKER_TEMPLATE_NAME }} \

            cd ~/online-auction-kubernetes
            sudo git add . && sudo git commit -m 'Commiting previous changes if any'
            sudo git pull --rebase origin main

            cd ..

            sudo -E ~/online-auction-kubernetes/self-managed/setup-gcp-ccm.sh

            cd ~/online-auction-kubernetes
            sudo git add . && sudo git commit -m 'Updating changes...'

  setup-csi:
    name: ğŸ’¾ Set up CSI Driver
    needs: [setup-cloud-controller, create-vms]
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout repo
        uses: actions/checkout@v3

      - name: ğŸ’» SSH into master and run CSI driver script
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ needs.create-vms.outputs.master_ip }}
          username: ${{ env.SSH_USER }}
          key: ${{ env.MASTER_SSH_KEY }}
          script: |
            export PROJECT_ID=${{ env.PROJECT_ID }}
            export ZONE=${{ env.ZONE }}

            sudo chmod +x ~/online-auction-kubernetes/self-managed/setup-csi-driver.sh
            sudo -E ~/online-auction-kubernetes/self-managed/setup-csi-driver.sh
